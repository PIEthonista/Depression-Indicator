{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is incomplete & has problems, never attemp to run, will crash your PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "089a09b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb5439e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this line:\n",
    "work_dir = \"/Users/gohyixian/Downloads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db29c4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_join</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>want go fun scared famely would find bi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>guys heard new restaurant moon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>republican party decide stands right stands se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>always told nala neighborhood dog old whenever...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>years ago audition second novel tsr 's brand n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_join  class\n",
       "0            want go fun scared famely would find bi      1\n",
       "1                     guys heard new restaurant moon      0\n",
       "2  republican party decide stands right stands se...      0\n",
       "3  always told nala neighborhood dog old whenever...      0\n",
       "4  years ago audition second novel tsr 's brand n...      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = work_dir + \"/NLP-Depression/Dataset/training_data.csv\"\n",
    "train_all = pd.read_csv(train_dir, index_col=False, encoding=\"utf-8\")\n",
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f08ae11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65529, 1)\n",
      "(65529, 3)\n",
      "(21844, 1)\n",
      "(21844, 3)\n",
      "[[0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train-test split, default: train=0.75, test=0.25\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_all[\"text_join\"], train_all[\"class\"], random_state=0)\n",
    "\n",
    "# some data type compatibility formatting\n",
    "x_train = np.asarray(x_train.to_frame().to_numpy(dtype=np.string_)).astype(np.string_)\n",
    "x_test = np.asarray(x_test.to_frame().to_numpy(dtype=np.string_)).astype(np.string_)\n",
    "y_train = np.asarray(y_train.to_frame().to_numpy(dtype=np.string_)).astype(np.string_)\n",
    "y_test = np.asarray(y_test.to_frame().to_numpy(dtype=np.string_)).astype(np.string_)\n",
    "\n",
    "\n",
    "# convert y data to labels of [0,1,0]\n",
    "# one-hot encoding\n",
    "def createLabels(y):\n",
    "    counter = 0\n",
    "    result = np.array([[]])\n",
    "    # initialize np.array based on first Training Example\n",
    "    if y[0] == np.string_(\"0\"):\n",
    "        result = np.array([[1,0,0]])\n",
    "    elif y[0] == np.string_(\"1\"):\n",
    "        result = np.array([[0,1,0]])\n",
    "    elif y[0] == np.string_(\"2\"):\n",
    "        result = np.array([[0,0,1]])\n",
    "    \n",
    "    # skip the first, process the other training examples\n",
    "    for i in y:\n",
    "        if counter > 0:\n",
    "            if i == np.string_(\"0\"):\n",
    "                result = np.append(result, np.array([[1,0,0]]), axis=0)\n",
    "            elif i == np.string_(\"1\"):\n",
    "                result = np.append(result, np.array([[0,1,0]]), axis=0)\n",
    "            elif i == np.string_(\"2\"):\n",
    "                result = np.append(result, np.array([[0,0,1]]), axis=0)\n",
    "        counter+=1\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "# x_train = x_train.to_frame()\n",
    "# print(type(x_train))\n",
    "# print(x_train)\n",
    "\n",
    "y_train = createLabels(y_train)\n",
    "y_test = createLabels(y_test)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b00688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5695\n"
     ]
    }
   ],
   "source": [
    "# review max text data length\n",
    "len_df = train_all[\"text_join\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "max_len = len_df.max()\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52df72b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'text_join'}>]], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "len_df_pd = pd.DataFrame(len_df)\n",
    "len_df_pd.hist(bins=10)\n",
    "# we can see that most data has len < 700, thus we will use\n",
    "# 1000 as the max text len for our BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20bffe29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'helppart wants end life part wants live last night got lucky tried end failed scared might end'],\n",
       "      dtype='|S32293')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1ff184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 03:41:17.490890: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-06-15 03:41:17.491383: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = \"bert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(pretrained_model_name,\n",
    "                                                            num_labels=4)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73ae182a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'download', '##ing', 'a', 'B', '##ER', '##T', 'model']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample use of tokenizer : sub-word embedding\n",
    "tokenizer.tokenize(\"I am downloading a BERT model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc0330c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# convert to python list\n",
    "# train_texts\n",
    "\n",
    "# train_texts = list(map((lambda x: x.decode(\"UTF-8\")), x_train.tolist()))\n",
    "train_texts = list(map((lambda x: str(x)), x_train.tolist()))\n",
    "test_texts = list(map((lambda x: str(x)), x_test.tolist()))\n",
    "\n",
    "\n",
    "#tokenize training data\n",
    "train_tokens = tokenizer(train_texts,\n",
    "                         max_length=700,              # max length of content, faster, save space too\n",
    "                         truncation=True,             # truncate words > 700\n",
    "                         padding=\"max_length\",        # pad shorter text to max_length\n",
    "                         add_special_tokens=True,     # special tokens like [CLS], start-of-line\n",
    "                         return_token_type_ids=False, # we don't need tokean type id in fine tuning here\n",
    "                         return_tensors=\"tf\")         # return tensorflow tensor\n",
    "\n",
    "#tokenize test data\n",
    "test_tokens = tokenizer(test_texts,\n",
    "                        max_length=700,              # max length of content, faster, save space too\n",
    "                        truncation=True,             # truncate words > 700\n",
    "                        padding=\"max_length\",        # pad shorter text to max_length\n",
    "                        add_special_tokens=True,     # special tokens like [CLS], start-of-line\n",
    "                        return_token_type_ids=False, # we don't need tokean type id in fine tuning here\n",
    "                        return_tensors=\"tf\")         # return tensorflow tensor\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1870afb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(65529, 700), dtype=int32, numpy=\n",
       "array([[101, 164, 171, ...,   0,   0,   0],\n",
       "       [101, 164, 171, ...,   0,   0,   0],\n",
       "       [101, 164, 171, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [101, 164, 171, ...,   0,   0,   0],\n",
       "       [101, 164, 171, ...,   0,   0,   0],\n",
       "       [101, 164, 171, ...,   0,   0,   0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(65529, 700), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e514d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe8742e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  101   164   171   112  1494 17482  1204  3349  1322  1297  1226  3349\n",
      "  1686  1314  1480  1400  6918  1793  1322  2604  5528  1547  1322   112\n",
      "   166   102     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0], shape=(700,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e85d6b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(700,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens[\"attention_mask\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8ba3a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'helppart wants end life part wants live last night got lucky tried end failed scared might end']\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd3eaea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=5e-5,\n",
    "                               epsilon=1e-8)    # epsilon to avoid zero div error\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=loss,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# freeze the BERT layer\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac9d2490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3076      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,313,348\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 108,310,272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4702a5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 19:41:02.502978: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot generate a hashable key for IteratorSpec(({'input_ids': TensorSpec(shape=(None, 700), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 700), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 3), dtype=tf.int64, name=None)),) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m log_dir \u001b[38;5;241m=\u001b[39m work_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/NLP-Depression/Logs/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlog_dir)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone babe ❤️😉\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m2/lib/python3.9/site-packages/tensorflow/core/function/polymorphism/function_cache.py:76\u001b[0m, in \u001b[0;36mFunctionCacheKey.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_signature\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m2/lib/python3.9/site-packages/tensorflow/core/function/trace_type/default_types.py:136\u001b[0m, in \u001b[0;36mOrderedCollection.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m2/lib/python3.9/site-packages/tensorflow/core/function/trace_type/default_types.py:136\u001b[0m, in \u001b[0;36mOrderedCollection.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m2/lib/python3.9/site-packages/tensorflow/core/function/trace_type/default_types.py:335\u001b[0m, in \u001b[0;36mReference.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 335\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot generate a hashable key for IteratorSpec(({'input_ids': TensorSpec(shape=(None, 700), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 700), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 3), dtype=tf.int64, name=None)),) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>"
     ]
    }
   ],
   "source": [
    "# this will take some time...🥹\n",
    "\n",
    "# Use TensorBoard to visualize metrics: loss & accuracy\n",
    "# NOTE: Clear any logs from previous runs\n",
    "# To save logs, remember to manually move them to the \"Logs Recorded\" folder\n",
    "# rm -rf ./Logs/\n",
    "train_tokens = tokenizer.decode(train_tokens)\n",
    "test_tokens = tokenizer.decode(test_tokens)\n",
    "\n",
    "\n",
    "log_dir = work_dir + \"/NLP-Depression/Logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model.fit(train_tokens, y_train,\n",
    "          epochs=3, \n",
    "          batch_size=100, \n",
    "          callbacks=[tensorboard_callback],\n",
    "          verbose=1,\n",
    "          validation_data=(test_tokens, y_test))\n",
    "\n",
    "print(\"Done babe ❤️😉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009f774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
